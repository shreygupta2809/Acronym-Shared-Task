{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"gpu-scibert-lm-fine-tuning.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xycs6CRXlQNP","executionInfo":{"elapsed":50097,"status":"ok","timestamp":1635176891519,"user":{"displayName":"Snehal Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhkjjevpEF4beZ78rH6BF90UtdYuR_odd0ycr-mRw=s64","userId":"09618462202303712708"},"user_tz":-240},"outputId":"a351ce29-16be-4586-b76d-c512f12a2831"},"source":["! pip install --upgrade pip -q\n","! pip install git+https://github.com/huggingface/transformers.git -q\n","! pip install git+https://github.com/huggingface/datasets.git -q"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[?25l\r\u001b[K     |▏                               | 10 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |▍                               | 20 kB 28.3 MB/s eta 0:00:01\r\u001b[K     |▋                               | 30 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |▊                               | 40 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |█                               | 51 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 61 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 71 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 81 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 92 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |██                              | 102 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██                              | 112 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 122 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 133 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 143 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 153 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███                             | 163 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 174 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 184 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 194 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 204 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████                            | 215 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 225 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 235 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 245 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 256 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████                           | 266 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 276 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 286 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 296 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 307 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████                          | 317 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████                          | 327 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 337 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 348 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 358 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 368 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████                         | 378 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 389 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 399 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 409 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 419 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████                        | 430 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 440 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 450 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 460 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 471 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 481 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 491 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 501 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 512 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 522 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 532 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 542 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 552 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 563 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 573 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 583 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 593 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 604 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 614 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 624 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 634 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 645 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 655 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 665 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 675 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 686 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 696 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 706 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 716 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 727 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 737 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 747 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 757 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 768 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 778 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 788 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 798 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 808 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 819 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 829 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 839 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 849 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 860 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 870 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 880 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 890 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 901 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 911 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 921 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 931 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 942 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 952 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 962 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 972 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 983 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 993 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 1.0 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 1.0 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.0 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 1.0 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.0 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 1.2 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.2 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 1.2 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.2 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.2 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.2 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.2 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.2 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.2 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.2 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.3 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.3 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.3 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.3 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.3 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.3 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.3 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.3 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.3 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.4 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.4 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.4 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.4 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 1.4 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.4 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.4 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.4 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.4 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.4 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.5 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.5 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.5 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.5 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.5 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.5 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.5 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.5 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.5 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.5 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.6 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.6 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.6 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.6 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.6 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.6 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.6 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.6 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.6 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.6 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.7 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.7 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.7 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.7 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.7 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.7 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.7 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.7 MB 5.3 MB/s \n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","     |████████████████████████████████| 596 kB 5.2 MB/s            \n","     |████████████████████████████████| 895 kB 30.3 MB/s            \n","     |████████████████████████████████| 56 kB 4.2 MB/s             \n","     |████████████████████████████████| 3.3 MB 32.0 MB/s            \n","\u001b[?25h  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","     |████████████████████████████████| 243 kB 5.2 MB/s            \n","     |████████████████████████████████| 125 kB 44.8 MB/s            \n","     |████████████████████████████████| 1.3 MB 33.2 MB/s            \n","     |████████████████████████████████| 271 kB 43.6 MB/s            \n","     |████████████████████████████████| 160 kB 47.2 MB/s            \n","\u001b[?25h  Building wheel for datasets (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NE3c5fsS_Eqf","executionInfo":{"elapsed":25043,"status":"ok","timestamp":1635176928772,"user":{"displayName":"Snehal Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhkjjevpEF4beZ78rH6BF90UtdYuR_odd0ycr-mRw=s64","userId":"09618462202303712708"},"user_tz":-240},"outputId":"4b2d3d53-f5fc-4fec-9fc0-ca15e7b71772"},"source":["from google.colab import drive\n","drive.mount('/gdrive')"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /gdrive\n"]}]},{"cell_type":"code","metadata":{"id":"VXOnvGlrt3U8"},"source":["! head -n 700000 /gdrive/My\\ Drive/IRE\\ Major\\ Project/AD/wiki_article.txt > train.txt\n","! tail -n 700000 /gdrive/My\\ Drive/IRE\\ Major\\ Project/AD/wiki_article.txt > val.txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qPYhpyRrKRNZ"},"source":["FOLDER = '/gdrive/My\\ Drive/IRE\\ Major\\ Project/AD/'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"Jyq4Bu3LAZBO","outputId":"22d636da-be1d-4baf-9d06-057bd62fb31b"},"source":["# ! rm -r /gdrive/My\\ Drive/SDU_2/scibert_scivocab_uncased_article && mkdir /gdrive/My\\ Drive/SDU_2/scibert_scivocab_uncased_article\n","\n","! python /gdrive/My\\ Drive/IRE\\ Major\\ Project/AD/mlm.py \\\n","    --model_name_or_path allenai/scibert_scivocab_uncased \\\n","    --train_file train.txt \\\n","    --validation_file val.txt \\\n","    --do_train \\\n","    --do_eval\\\n","    --output_dir /gdrive/My\\ Drive/IRE\\ Major\\ Project/AD/finetune \\\n","    --line_by_line \\\n","    --max_seq_length 256\\\n","    --num_train_epochs 1\\\n","    --per_device_train_batch_size 16\\\n","    --per_device_eval_batch_size 16\\"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["10/25/2021 16:07:53 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","10/25/2021 16:07:53 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_find_unused_parameters=None,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_steps=None,\n","evaluation_strategy=IntervalStrategy.NO,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=None,\n","group_by_length=False,\n","hub_model_id=None,\n","hub_strategy=HubStrategy.EVERY_SAVE,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=-1,\n","log_level=-1,\n","log_level_replica=-1,\n","log_on_each_node=True,\n","logging_dir=/gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/runs/Oct25_16-07-53_bc7f328ac11f,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=IntervalStrategy.STEPS,\n","lr_scheduler_type=SchedulerType.LINEAR,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=1.0,\n","output_dir=/gdrive/My Drive/IRE Major Project/AD/SciDr/finetune,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=16,\n","per_device_train_batch_size=16,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=/gdrive/My Drive/IRE Major Project/AD/SciDr/finetune,\n","save_on_each_node=False,\n","save_steps=500,\n","save_strategy=IntervalStrategy.STEPS,\n","save_total_limit=None,\n","seed=42,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_legacy_prediction_loop=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n","xpu_backend=None,\n",")\n","10/25/2021 16:07:53 - WARNING - datasets.builder - Using custom data configuration default-aea03c286355f7b7\n","10/25/2021 16:07:53 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n","10/25/2021 16:07:53 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-aea03c286355f7b7/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\n","10/25/2021 16:07:53 - WARNING - datasets.builder - Reusing dataset text (/root/.cache/huggingface/datasets/text/default-aea03c286355f7b7/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\n","10/25/2021 16:07:53 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-aea03c286355f7b7/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\n","100% 2/2 [00:00<00:00, 524.62it/s]\n","10/25/2021 16:07:53 - DEBUG - filelock - Attempting to acquire lock 139865792222224 on /root/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd.lock\n","10/25/2021 16:07:53 - DEBUG - filelock - Lock 139865792222224 acquired on /root/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd.lock\n","[INFO|file_utils.py:1753] 2021-10-25 16:07:53,498 >> https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp_z4t4m8f\n","Downloading: 100% 385/385 [00:00<00:00, 369kB/s]\n","[INFO|file_utils.py:1757] 2021-10-25 16:07:53,629 >> storing https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n","[INFO|file_utils.py:1765] 2021-10-25 16:07:53,629 >> creating metadata file for /root/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n","10/25/2021 16:07:53 - DEBUG - filelock - Attempting to release lock 139865792222224 on /root/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd.lock\n","10/25/2021 16:07:53 - DEBUG - filelock - Lock 139865792222224 released on /root/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd.lock\n","[INFO|configuration_utils.py:588] 2021-10-25 16:07:53,629 >> loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n","[INFO|configuration_utils.py:625] 2021-10-25 16:07:53,630 >> Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 31090\n","}\n","\n","[INFO|tokenization_auto.py:341] 2021-10-25 16:07:53,763 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n","[INFO|configuration_utils.py:588] 2021-10-25 16:07:53,893 >> loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n","[INFO|configuration_utils.py:625] 2021-10-25 16:07:53,894 >> Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 31090\n","}\n","\n","10/25/2021 16:07:54 - DEBUG - filelock - Attempting to acquire lock 139865735684816 on /root/.cache/huggingface/transformers/33593020f507d72099bd84ea6cd2296feb424fecd62d4a8edcc2a02899af6e29.38339d84e6e392addd730fd85fae32652c4cc7c5423633d6fa73e5f7937bbc38.lock\n","10/25/2021 16:07:54 - DEBUG - filelock - Lock 139865735684816 acquired on /root/.cache/huggingface/transformers/33593020f507d72099bd84ea6cd2296feb424fecd62d4a8edcc2a02899af6e29.38339d84e6e392addd730fd85fae32652c4cc7c5423633d6fa73e5f7937bbc38.lock\n","[INFO|file_utils.py:1753] 2021-10-25 16:07:54,152 >> https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpvt663v2y\n","Downloading: 100% 223k/223k [00:00<00:00, 2.79MB/s]\n","[INFO|file_utils.py:1757] 2021-10-25 16:07:54,388 >> storing https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/33593020f507d72099bd84ea6cd2296feb424fecd62d4a8edcc2a02899af6e29.38339d84e6e392addd730fd85fae32652c4cc7c5423633d6fa73e5f7937bbc38\n","[INFO|file_utils.py:1765] 2021-10-25 16:07:54,388 >> creating metadata file for /root/.cache/huggingface/transformers/33593020f507d72099bd84ea6cd2296feb424fecd62d4a8edcc2a02899af6e29.38339d84e6e392addd730fd85fae32652c4cc7c5423633d6fa73e5f7937bbc38\n","10/25/2021 16:07:54 - DEBUG - filelock - Attempting to release lock 139865735684816 on /root/.cache/huggingface/transformers/33593020f507d72099bd84ea6cd2296feb424fecd62d4a8edcc2a02899af6e29.38339d84e6e392addd730fd85fae32652c4cc7c5423633d6fa73e5f7937bbc38.lock\n","10/25/2021 16:07:54 - DEBUG - filelock - Lock 139865735684816 released on /root/.cache/huggingface/transformers/33593020f507d72099bd84ea6cd2296feb424fecd62d4a8edcc2a02899af6e29.38339d84e6e392addd730fd85fae32652c4cc7c5423633d6fa73e5f7937bbc38.lock\n","[INFO|tokenization_utils_base.py:1742] 2021-10-25 16:07:54,908 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/33593020f507d72099bd84ea6cd2296feb424fecd62d4a8edcc2a02899af6e29.38339d84e6e392addd730fd85fae32652c4cc7c5423633d6fa73e5f7937bbc38\n","[INFO|tokenization_utils_base.py:1742] 2021-10-25 16:07:54,908 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:1742] 2021-10-25 16:07:54,909 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1742] 2021-10-25 16:07:54,909 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1742] 2021-10-25 16:07:54,909 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/tokenizer_config.json from cache at None\n","[INFO|configuration_utils.py:588] 2021-10-25 16:07:55,040 >> loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n","[INFO|configuration_utils.py:625] 2021-10-25 16:07:55,040 >> Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 31090\n","}\n","\n","[INFO|configuration_utils.py:588] 2021-10-25 16:07:55,316 >> loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n","[INFO|configuration_utils.py:625] 2021-10-25 16:07:55,316 >> Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 31090\n","}\n","\n","10/25/2021 16:07:55 - DEBUG - filelock - Attempting to acquire lock 139865656242576 on /root/.cache/huggingface/transformers/de14937a851e8180a2bc5660c0041d385f8a0c62b1b2ccafa46df31043a2390c.74830bb01a0ffcdeaed8be9916312726d0c4cd364ac6fc15b375f789eaff4cbb.lock\n","10/25/2021 16:07:55 - DEBUG - filelock - Lock 139865656242576 acquired on /root/.cache/huggingface/transformers/de14937a851e8180a2bc5660c0041d385f8a0c62b1b2ccafa46df31043a2390c.74830bb01a0ffcdeaed8be9916312726d0c4cd364ac6fc15b375f789eaff4cbb.lock\n","[INFO|file_utils.py:1753] 2021-10-25 16:07:55,480 >> https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpvkm8rnl7\n","Downloading: 100% 422M/422M [00:19<00:00, 22.1MB/s]\n","[INFO|file_utils.py:1757] 2021-10-25 16:08:15,628 >> storing https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/de14937a851e8180a2bc5660c0041d385f8a0c62b1b2ccafa46df31043a2390c.74830bb01a0ffcdeaed8be9916312726d0c4cd364ac6fc15b375f789eaff4cbb\n","[INFO|file_utils.py:1765] 2021-10-25 16:08:15,629 >> creating metadata file for /root/.cache/huggingface/transformers/de14937a851e8180a2bc5660c0041d385f8a0c62b1b2ccafa46df31043a2390c.74830bb01a0ffcdeaed8be9916312726d0c4cd364ac6fc15b375f789eaff4cbb\n","10/25/2021 16:08:15 - DEBUG - filelock - Attempting to release lock 139865656242576 on /root/.cache/huggingface/transformers/de14937a851e8180a2bc5660c0041d385f8a0c62b1b2ccafa46df31043a2390c.74830bb01a0ffcdeaed8be9916312726d0c4cd364ac6fc15b375f789eaff4cbb.lock\n","10/25/2021 16:08:15 - DEBUG - filelock - Lock 139865656242576 released on /root/.cache/huggingface/transformers/de14937a851e8180a2bc5660c0041d385f8a0c62b1b2ccafa46df31043a2390c.74830bb01a0ffcdeaed8be9916312726d0c4cd364ac6fc15b375f789eaff4cbb.lock\n","[INFO|modeling_utils.py:1340] 2021-10-25 16:08:15,629 >> loading weights file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/de14937a851e8180a2bc5660c0041d385f8a0c62b1b2ccafa46df31043a2390c.74830bb01a0ffcdeaed8be9916312726d0c4cd364ac6fc15b375f789eaff4cbb\n","[WARNING|modeling_utils.py:1599] 2021-10-25 16:08:17,586 >> Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[INFO|modeling_utils.py:1616] 2021-10-25 16:08:17,586 >> All the weights of BertForMaskedLM were initialized from the model checkpoint at allenai/scibert_scivocab_uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n","Running tokenizer on dataset line_by_line:   0% 0/700 [00:00<?, ?ba/s]10/25/2021 16:08:17 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-aea03c286355f7b7/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5/cache-668dc409e32f13fc.arrow\n","Running tokenizer on dataset line_by_line: 100% 700/700 [00:22<00:00, 31.09ba/s]\n","Running tokenizer on dataset line_by_line:   0% 0/700 [00:00<?, ?ba/s]10/25/2021 16:08:40 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-aea03c286355f7b7/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5/cache-23d3de9b40c9ca8a.arrow\n","Running tokenizer on dataset line_by_line: 100% 700/700 [00:24<00:00, 28.85ba/s]\n","[INFO|trainer.py:541] 2021-10-25 16:09:24,138 >> The following columns in the training set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n","[INFO|trainer.py:1196] 2021-10-25 16:09:24,154 >> ***** Running training *****\n","[INFO|trainer.py:1197] 2021-10-25 16:09:24,154 >>   Num examples = 330655\n","[INFO|trainer.py:1198] 2021-10-25 16:09:24,154 >>   Num Epochs = 1\n","[INFO|trainer.py:1199] 2021-10-25 16:09:24,154 >>   Instantaneous batch size per device = 16\n","[INFO|trainer.py:1200] 2021-10-25 16:09:24,154 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n","[INFO|trainer.py:1201] 2021-10-25 16:09:24,154 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1202] 2021-10-25 16:09:24,154 >>   Total optimization steps = 20666\n","{'loss': 2.3548, 'learning_rate': 4.8790283557534114e-05, 'epoch': 0.02}\n","  2% 500/20666 [07:38<5:58:48,  1.07s/it][INFO|trainer.py:1995] 2021-10-25 16:17:02,462 >> Saving model checkpoint to /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-500\n","[INFO|configuration_utils.py:417] 2021-10-25 16:17:02,471 >> Configuration saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-500/config.json\n","[INFO|modeling_utils.py:1058] 2021-10-25 16:17:04,882 >> Model weights saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2034] 2021-10-25 16:17:04,897 >> tokenizer config file saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2040] 2021-10-25 16:17:04,903 >> Special tokens file saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-500/special_tokens_map.json\n","{'loss': 2.254, 'learning_rate': 4.758056711506823e-05, 'epoch': 0.05}\n","  5% 1000/20666 [15:26<5:03:26,  1.08it/s][INFO|trainer.py:1995] 2021-10-25 16:24:50,553 >> Saving model checkpoint to /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-1000\n","[INFO|configuration_utils.py:417] 2021-10-25 16:24:50,568 >> Configuration saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-1000/config.json\n","[INFO|modeling_utils.py:1058] 2021-10-25 16:24:52,879 >> Model weights saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-1000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2034] 2021-10-25 16:24:53,302 >> tokenizer config file saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2040] 2021-10-25 16:24:53,321 >> Special tokens file saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-1000/special_tokens_map.json\n","{'loss': 2.2737, 'learning_rate': 4.6370850672602345e-05, 'epoch': 0.07}\n","  7% 1500/20666 [23:29<6:20:10,  1.19s/it][INFO|trainer.py:1995] 2021-10-25 16:32:53,780 >> Saving model checkpoint to /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-1500\n","[INFO|configuration_utils.py:417] 2021-10-25 16:32:53,790 >> Configuration saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-1500/config.json\n","[INFO|modeling_utils.py:1058] 2021-10-25 16:32:56,072 >> Model weights saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-1500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2034] 2021-10-25 16:32:56,083 >> tokenizer config file saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2040] 2021-10-25 16:32:56,089 >> Special tokens file saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-1500/special_tokens_map.json\n","{'loss': 2.2453, 'learning_rate': 4.5161134230136456e-05, 'epoch': 0.1}\n"," 10% 2000/20666 [31:26<5:46:44,  1.11s/it][INFO|trainer.py:1995] 2021-10-25 16:40:50,269 >> Saving model checkpoint to /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-2000\n","[INFO|configuration_utils.py:417] 2021-10-25 16:40:50,279 >> Configuration saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-2000/config.json\n","[INFO|modeling_utils.py:1058] 2021-10-25 16:40:52,513 >> Model weights saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-2000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2034] 2021-10-25 16:40:52,528 >> tokenizer config file saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2040] 2021-10-25 16:40:52,539 >> Special tokens file saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-2000/special_tokens_map.json\n","{'loss': 2.198, 'learning_rate': 4.3951417787670575e-05, 'epoch': 0.12}\n"," 12% 2500/20666 [39:27<4:28:09,  1.13it/s][INFO|trainer.py:1995] 2021-10-25 16:48:51,226 >> Saving model checkpoint to /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-2500\n","[INFO|configuration_utils.py:417] 2021-10-25 16:48:51,238 >> Configuration saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-2500/config.json\n","[INFO|modeling_utils.py:1058] 2021-10-25 16:48:53,533 >> Model weights saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-2500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2034] 2021-10-25 16:48:53,550 >> tokenizer config file saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2040] 2021-10-25 16:48:53,560 >> Special tokens file saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-2500/special_tokens_map.json\n","{'loss': 2.2763, 'learning_rate': 4.274170134520469e-05, 'epoch': 0.15}\n"," 15% 3000/20666 [47:28<4:06:03,  1.20it/s][INFO|trainer.py:1995] 2021-10-25 16:56:52,959 >> Saving model checkpoint to /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-3000\n","[INFO|configuration_utils.py:417] 2021-10-25 16:56:52,969 >> Configuration saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-3000/config.json\n","[INFO|modeling_utils.py:1058] 2021-10-25 16:56:55,336 >> Model weights saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-3000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2034] 2021-10-25 16:56:55,349 >> tokenizer config file saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-3000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2040] 2021-10-25 16:56:55,355 >> Special tokens file saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-3000/special_tokens_map.json\n","{'loss': 2.2195, 'learning_rate': 4.1531984902738805e-05, 'epoch': 0.17}\n"," 17% 3500/20666 [55:31<3:25:30,  1.39it/s][INFO|trainer.py:1995] 2021-10-25 17:04:56,018 >> Saving model checkpoint to /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-3500\n","[INFO|configuration_utils.py:417] 2021-10-25 17:04:56,028 >> Configuration saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-3500/config.json\n","[INFO|modeling_utils.py:1058] 2021-10-25 17:04:58,340 >> Model weights saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-3500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2034] 2021-10-25 17:04:58,361 >> tokenizer config file saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-3500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2040] 2021-10-25 17:04:58,369 >> Special tokens file saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-3500/special_tokens_map.json\n","{'loss': 2.2579, 'learning_rate': 4.032226846027292e-05, 'epoch': 0.19}\n"," 19% 4000/20666 [1:03:00<3:33:34,  1.30it/s][INFO|trainer.py:1995] 2021-10-25 17:12:24,743 >> Saving model checkpoint to /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-4000\n","[INFO|configuration_utils.py:417] 2021-10-25 17:12:24,754 >> Configuration saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-4000/config.json\n","[INFO|modeling_utils.py:1058] 2021-10-25 17:12:27,075 >> Model weights saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-4000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2034] 2021-10-25 17:12:27,083 >> tokenizer config file saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-4000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2040] 2021-10-25 17:12:27,096 >> Special tokens file saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-4000/special_tokens_map.json\n","{'loss': 2.1738, 'learning_rate': 3.911255201780703e-05, 'epoch': 0.22}\n"," 22% 4500/20666 [1:10:46<3:32:15,  1.27it/s][INFO|trainer.py:1995] 2021-10-25 17:20:10,343 >> Saving model checkpoint to /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-4500\n","[INFO|configuration_utils.py:417] 2021-10-25 17:20:10,354 >> Configuration saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-4500/config.json\n","[INFO|modeling_utils.py:1058] 2021-10-25 17:20:12,782 >> Model weights saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-4500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2034] 2021-10-25 17:20:12,791 >> tokenizer config file saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-4500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2040] 2021-10-25 17:20:12,797 >> Special tokens file saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-4500/special_tokens_map.json\n","{'loss': 2.2035, 'learning_rate': 3.790283557534115e-05, 'epoch': 0.24}\n"," 24% 5000/20666 [1:18:55<2:27:34,  1.77it/s][INFO|trainer.py:1995] 2021-10-25 17:28:19,944 >> Saving model checkpoint to /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-5000\n","[INFO|configuration_utils.py:417] 2021-10-25 17:28:19,956 >> Configuration saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-5000/config.json\n","[INFO|modeling_utils.py:1058] 2021-10-25 17:28:22,322 >> Model weights saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-5000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2034] 2021-10-25 17:28:22,333 >> tokenizer config file saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-5000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2040] 2021-10-25 17:28:22,615 >> Special tokens file saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-5000/special_tokens_map.json\n","{'loss': 2.1493, 'learning_rate': 3.669311913287525e-05, 'epoch': 0.27}\n"," 27% 5500/20666 [1:26:50<4:56:12,  1.17s/it][INFO|trainer.py:1995] 2021-10-25 17:36:14,807 >> Saving model checkpoint to /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-5500\n","[INFO|configuration_utils.py:417] 2021-10-25 17:36:14,817 >> Configuration saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-5500/config.json\n","[INFO|modeling_utils.py:1058] 2021-10-25 17:36:17,126 >> Model weights saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-5500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2034] 2021-10-25 17:36:17,137 >> tokenizer config file saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-5500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2040] 2021-10-25 17:36:17,143 >> Special tokens file saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-5500/special_tokens_map.json\n","{'loss': 2.14, 'learning_rate': 3.5483402690409364e-05, 'epoch': 0.29}\n"," 29% 6000/20666 [1:34:43<3:46:36,  1.08it/s][INFO|trainer.py:1995] 2021-10-25 17:44:07,540 >> Saving model checkpoint to /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-6000\n","[INFO|configuration_utils.py:417] 2021-10-25 17:44:07,553 >> Configuration saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-6000/config.json\n","[INFO|modeling_utils.py:1058] 2021-10-25 17:44:09,909 >> Model weights saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-6000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2034] 2021-10-25 17:44:09,918 >> tokenizer config file saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-6000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2040] 2021-10-25 17:44:09,926 >> Special tokens file saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-6000/special_tokens_map.json\n","{'loss': 2.1466, 'learning_rate': 3.427368624794348e-05, 'epoch': 0.31}\n"," 31% 6500/20666 [1:41:57<4:05:17,  1.04s/it][INFO|trainer.py:1995] 2021-10-25 17:51:21,979 >> Saving model checkpoint to /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-6500\n","[INFO|configuration_utils.py:417] 2021-10-25 17:51:21,990 >> Configuration saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-6500/config.json\n","[INFO|modeling_utils.py:1058] 2021-10-25 17:51:24,301 >> Model weights saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-6500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2034] 2021-10-25 17:51:24,310 >> tokenizer config file saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-6500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2040] 2021-10-25 17:51:24,317 >> Special tokens file saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-6500/special_tokens_map.json\n","{'loss': 2.1543, 'learning_rate': 3.3063969805477595e-05, 'epoch': 0.34}\n"," 34% 7000/20666 [1:49:49<2:59:29,  1.27it/s][INFO|trainer.py:1995] 2021-10-25 17:59:13,632 >> Saving model checkpoint to /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-7000\n","[INFO|configuration_utils.py:417] 2021-10-25 17:59:13,641 >> Configuration saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-7000/config.json\n","[INFO|modeling_utils.py:1058] 2021-10-25 17:59:16,021 >> Model weights saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-7000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2034] 2021-10-25 17:59:16,045 >> tokenizer config file saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-7000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2040] 2021-10-25 17:59:16,050 >> Special tokens file saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-7000/special_tokens_map.json\n","{'loss': 2.2123, 'learning_rate': 3.185425336301171e-05, 'epoch': 0.36}\n"," 36% 7500/20666 [1:57:51<4:31:46,  1.24s/it][INFO|trainer.py:1995] 2021-10-25 18:07:15,709 >> Saving model checkpoint to /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-7500\n","[INFO|configuration_utils.py:417] 2021-10-25 18:07:15,720 >> Configuration saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-7500/config.json\n","[INFO|modeling_utils.py:1058] 2021-10-25 18:07:18,080 >> Model weights saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-7500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2034] 2021-10-25 18:07:18,094 >> tokenizer config file saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-7500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2040] 2021-10-25 18:07:18,099 >> Special tokens file saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-7500/special_tokens_map.json\n","{'loss': 2.0921, 'learning_rate': 3.0644536920545825e-05, 'epoch': 0.39}\n"," 39% 8000/20666 [2:05:50<3:18:33,  1.06it/s][INFO|trainer.py:1995] 2021-10-25 18:15:14,282 >> Saving model checkpoint to /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-8000\n","[INFO|configuration_utils.py:417] 2021-10-25 18:15:14,292 >> Configuration saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-8000/config.json\n","[INFO|modeling_utils.py:1058] 2021-10-25 18:15:16,616 >> Model weights saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-8000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2034] 2021-10-25 18:15:16,891 >> tokenizer config file saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-8000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2040] 2021-10-25 18:15:16,905 >> Special tokens file saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-8000/special_tokens_map.json\n","{'loss': 2.1439, 'learning_rate': 2.943482047807994e-05, 'epoch': 0.41}\n"," 41% 8500/20666 [2:13:38<3:55:52,  1.16s/it][INFO|trainer.py:1995] 2021-10-25 18:23:03,049 >> Saving model checkpoint to /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-8500\n","[INFO|configuration_utils.py:417] 2021-10-25 18:23:03,062 >> Configuration saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-8500/config.json\n","[INFO|modeling_utils.py:1058] 2021-10-25 18:23:05,554 >> Model weights saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-8500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2034] 2021-10-25 18:23:05,561 >> tokenizer config file saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-8500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2040] 2021-10-25 18:23:05,566 >> Special tokens file saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-8500/special_tokens_map.json\n","{'loss': 2.1215, 'learning_rate': 2.8225104035614052e-05, 'epoch': 0.44}\n"," 44% 9000/20666 [2:21:29<2:47:54,  1.16it/s][INFO|trainer.py:1995] 2021-10-25 18:30:53,751 >> Saving model checkpoint to /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-9000\n","[INFO|configuration_utils.py:417] 2021-10-25 18:30:53,761 >> Configuration saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-9000/config.json\n","[INFO|modeling_utils.py:1058] 2021-10-25 18:30:56,115 >> Model weights saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-9000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2034] 2021-10-25 18:30:56,125 >> tokenizer config file saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-9000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2040] 2021-10-25 18:30:56,133 >> Special tokens file saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-9000/special_tokens_map.json\n","{'loss': 2.0659, 'learning_rate': 2.7015387593148167e-05, 'epoch': 0.46}\n"," 46% 9500/20666 [2:29:17<3:14:31,  1.05s/it][INFO|trainer.py:1995] 2021-10-25 18:38:42,122 >> Saving model checkpoint to /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-9500\n","[INFO|configuration_utils.py:417] 2021-10-25 18:38:42,135 >> Configuration saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-9500/config.json\n","[INFO|modeling_utils.py:1058] 2021-10-25 18:38:44,508 >> Model weights saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-9500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2034] 2021-10-25 18:38:44,517 >> tokenizer config file saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-9500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2040] 2021-10-25 18:38:44,525 >> Special tokens file saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-9500/special_tokens_map.json\n","{'loss': 2.0365, 'learning_rate': 2.5805671150682282e-05, 'epoch': 0.48}\n"," 48% 10000/20666 [2:37:27<2:07:58,  1.39it/s][INFO|trainer.py:1995] 2021-10-25 18:46:51,833 >> Saving model checkpoint to /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-10000\n","[INFO|configuration_utils.py:417] 2021-10-25 18:46:51,843 >> Configuration saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-10000/config.json\n","[INFO|modeling_utils.py:1058] 2021-10-25 18:46:54,233 >> Model weights saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-10000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2034] 2021-10-25 18:46:54,667 >> tokenizer config file saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-10000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2040] 2021-10-25 18:46:54,677 >> Special tokens file saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-10000/special_tokens_map.json\n","{'loss': 2.0819, 'learning_rate': 2.4595954708216394e-05, 'epoch': 0.51}\n"," 51% 10500/20666 [2:45:18<1:47:41,  1.57it/s][INFO|trainer.py:1995] 2021-10-25 18:54:42,607 >> Saving model checkpoint to /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-10500\n","[INFO|configuration_utils.py:417] 2021-10-25 18:54:42,616 >> Configuration saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-10500/config.json\n","[INFO|modeling_utils.py:1058] 2021-10-25 18:54:44,880 >> Model weights saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-10500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2034] 2021-10-25 18:54:44,890 >> tokenizer config file saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-10500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2040] 2021-10-25 18:54:44,896 >> Special tokens file saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-10500/special_tokens_map.json\n","{'loss': 2.0706, 'learning_rate': 2.338623826575051e-05, 'epoch': 0.53}\n"," 53% 11000/20666 [2:52:44<3:01:54,  1.13s/it][INFO|trainer.py:1995] 2021-10-25 19:02:09,028 >> Saving model checkpoint to /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-11000\n","[INFO|configuration_utils.py:417] 2021-10-25 19:02:09,040 >> Configuration saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-11000/config.json\n","[INFO|modeling_utils.py:1058] 2021-10-25 19:02:11,403 >> Model weights saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-11000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2034] 2021-10-25 19:02:11,412 >> tokenizer config file saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-11000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2040] 2021-10-25 19:02:11,421 >> Special tokens file saved in /gdrive/My Drive/IRE Major Project/AD/SciDr/finetune/checkpoint-11000/special_tokens_map.json\n"," 55% 11362/20666 [2:58:23<2:00:34,  1.29it/s]"]}]},{"cell_type":"code","metadata":{"id":"tGYyDc-AzMOQ"},"source":[""],"execution_count":null,"outputs":[]}]}